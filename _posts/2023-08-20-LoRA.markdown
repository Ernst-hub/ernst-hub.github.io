---
layout: post
comments: true
title: "Low-Rank Adaptation (LoRA): A Deep Dive into Efficient Model Tuning"
excerpt: "Delving into the mechanics and benefits of LoRA for effective and efficient tuning of large neural networks."
date: 2023-08-20 22:00:00
category: "NLP"
mathjax: true
---

# LoRA

# Background

LoRA takes inspiration from a previous paper that shows that extremely large “overparameterized” models reside on a low intrinsic dimension. 

Even though a neural network might have millions of parameters, the actual "degrees of freedom" that the model needs to capture the underlying data distribution could be much lower.

LoRA takes this a step further, by hypothesising that it is not just the pertained model itself, but also the change in weights during model adaptation for down stream tasks that has a low intrinsic rank. 

# How exactly does LoRA work?

1. LoRA starts with a pre-trained NN, it then freezes the weights of the model.
2. Instead of updating the weights of some dense layers, LoRA optimises rank decomposition matrices that represent the change in these dense layers.
3. Hence, LoRA is essentially training these layers indirectly. It's like saying, "I won't touch the original masterpiece, but I'll add a few brush strokes that capture the essence of what needs to change.”

The benefit here is that you can make meaningful adaptations to the model without disturbing the pre-trained weights, and you can do this efficiently by focusing on the low-rank structure of the changes.

So, in a nutshell, LoRA is like a savvy hacker who knows exactly where to inject a few lines of code to make the whole program run better, without messing up the existing architecture.

## Reference to manifold theory

Some of my previous notes touched upon the notion of manifolds:

[Recap 1: Constructing manifolds?](../../math/topology_and_manifolds/recap_manifolds.md)

It is conceptually rewarding to understand the possible parameter space of the model as vast and high-dimensional. The pre-trained model has then approximated a manifold in this space. This manifold is of course based on the training input. For downstream tasks, there need to be some adaptation, i.e. fine-tuning. LoRA is essentially approximating the manifold and making efficient adaptations to the model without straying far from this manifold. LoRA is like a navigator that already has a good map (the pre-trained model) and is making precise, efficient updates to reach a new destination (adaptation) without getting lost in the high-dimensional wilderness.

## Low-Rank decomposition matrices

A fully connected layer can be represented as a weight matrix \\(W \in \mathbb{R}^{d\times h}\\), where \\(d\\) is the input dimension and \\(h\\) the output dimension.

A low rank approximation:

\\(W \approx U \times V\\), where \\(U \in \mathbb{R}^{d \times r}\\) and \\(V \in \mathbb{R}^{r \times h}\\), here \\(r\\) is the rank and is much smaller than \\(d\\) and \\(h\\).

### How LoRA uses these matrices:

1. Take pre-trained model
2. Identify dense layers to adapt for downstream task
3. Optimise low rank matrices: Only update \\(U\\) and \\(V\\).
4. Apply changes in forward and backward pass: Rubin training the weight matrix becomes: \\(W + U \times V\\)

**Downstream task:**

1. Do a forward pass with \\(W + U\times V\\)
2. Do the backward pass
3. Step the optimizer, but only update \\(U\\) and \\(V\\)
4. Once training is complete, deploy the model with the effective weight matrices \\(W + U \times V\\)

# Benefits:

- A pre-trained model can be shared and used to build many small LoRA modules for dif- ferent tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices A and B in Figure 1, reducing the storage requirement and task-switching over- head significantly.
- LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices.
- Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, *introducing no inference latency* compared to a fully fine-tuned model, by construction.

The paper turns to answer the following RELEVANT questions:

1. Given a parameter budget constraint, which subsets of weight matrices in a pre-trained transformer should we adapt to maximise downstream performance?
2. Is the optimal adaptation matrix \\(\nabla W\\) really ********rank-deficient?********
	1. IF SO what is a good rank \\(r\\) to use in practice?
3. What is the connection between \\(W\\) and \\(\nabla W\\)? Does \\(\nabla W\\) highly correlate with \\(W\\)? how large is \\(\nabla W\\) compared to \\(W\\)

# Extended research questions

All answers are founded empirically, not theoretically proven. The results are derived from an analysis on GPT-3-175B.

## **1) which weight matrices to use under budget constraint?**

![zoomed image](assets/nlp/Untitled.png){: .zoomed}

Generally, using both Wq and Wv is better than only using one of them. 

That means, lower rank \\(r\\) but more matrices are better than one matrix with a high rank. 

## **2) the optimal rank \\(r\\)?**

![zoomed image](assets/nlp/Untitled 1.png){: .zoomed}

We see that the \\(\nabla W\\) already performs super well on a very low rank. This indicates that \\(\nabla W\\) might have a very small intrinsic rank. 

To support this claim, the authors dive into analysing the overlap of the subspaces learned by different choices of \\(r\\) and by different random seeds.

To do this, we use SVD!

> Given the adaptation matrices \\(A_{r=8}\\) and \\(A_{r=64}\\) using the same pre-trained model, we perform singular value decomposition and obtain the right-singular unitary matrices \\(U_{A_{r=8} }\\) and \\(U_{A_{r=64} }\\).

How much of the subspace spanned by the top \\(I\\) singular vectors in \\(U_{A_{r=8} }\\) for \\(1 \leq i \leq 8\\) is contained in the subspace spanned by the top \\(j\\) singular vectors of \\(U_{A_{r=64} }\\) for \\(1 \leq j \leq 64\\)?

This is measured by the normalised subspace similarity based on the Grassmann distance:

 

\\(\\)
\phi(A_{r=8}, A_{r=64},i,j) = \frac{\left\| U^{i^T}_{A_{r=8} } U^{j}_{A_{r=64} }\right\|^2_F}{\min(i,j)}
\\(\\)

Where \\(U^i_{A_{r=8} }\\) represents the columns of \\(U_{A_{r=8} }\\) corresponding to the top-i singular vectors.

\\(\phi(.)\\) has a range of \\([0,1]\\) where \\(1\\) means complete overlap and \\(0\\) complete separation.

This figure shows the 48th layer of the 96 layers in the transformer.

![zoomed image](assets/nlp/Untitled 2.png){: .zoomed}

By looking at the figure, the authors state: “Directions corresponding to the top singular vector overlap significantly between \\(A_{r=8}\\) and \\(A_{r=64}\\), while others do not.”

The figure indicates that the top singular-vector directions of the two adaptation matrices are the most useful while the other directions potentially contain mostly random noises accumulated during training. Therefore, the adaptation matrix can have a very low rank.

The focus is on comparing subspaces of the same dimensionality or how a larger subspace contains a smaller one, not the other way around. That is why the there are grey blocks for j > i.

Across different random seeds:

![zoomed image](assets/nlp/Untitled 3.png){: .zoomed}

\\(\nabla W_q\\) appears to have a higher intrinsic rank than \\(\nabla W_v\\), since more common singular value directions are learned by both runs for \\(\nabla W_q\\)

## **3) How does the adaptation matrix compare to the pre-trained weight matrix?**

TO answer these questions we project \\(W\\) onto the \\(r\\)-dimensional subspace of \\(\nabla W\\). This is done by computing:

\\(\\)
U^TWV^T
\\(\\)

Here U and V are the left and right singular vector matrices of \\(\nabla W\\).

Once this is computed, we can compare the Frobenius norm between: \\(\|U^TWV^T\|_F\\) and \\(\|W \|_F\\). For comparisons, we can also compute \\(\|U^TWV^T\|_F\\) by replacing \\(U, V\\) with the top \\(r\\) singular vectors of \\(W\\) itself or a random matrix.

![zoomed image](assets/nlp/Untitled 4.png){: .zoomed}

- It shows that \\(\nabla W\\) has a stronger correlation with \\(W\\) compared to a random matrix. Hence \\(\nabla W\\) amplifies some features already present in \\(W\\).
- Second, instead of repeating the top singular directions of \\(W\\), \\(\nabla W\\) only amplifies directions that are not emphasized in \\(W\\).
- Third, the amplification factor is huge: \\(21.5 \approx 6.91 / 0.32\\) for \\(r = 4\\). This suggests that the low-rank adaptation potentially amplifies the important features for specific downstream tasks that were learned but not emphasised in the general pre-training model.